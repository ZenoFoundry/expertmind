# ğŸ§  ExpertMind

**Sistema de Inteligencia Artificial para AnÃ¡lisis de Negocio y Toma de Decisiones**

ExpertMind es un sistema de inteligencia artificial diseÃ±ado para responder preguntas de negocio complejas y ofrecer anÃ¡lisis sobre el impacto de ciertas acciones, basÃ¡ndose en una base de conocimiento construida a partir de documentaciÃ³n y cÃ³digo fuente. El sistema utiliza modelos de IA locales (Ollama) integrados con repositorios de GitHub mediante el protocolo MCP (Model Context Protocol).

## ğŸ¯ Objetivos del Proyecto

- **AnÃ¡lisis Inteligente**: Procesar y analizar documentaciÃ³n tÃ©cnica y de negocio para generar insights accionables
- **Consultas Contextuales**: Responder preguntas especÃ­ficas sobre proyectos, cÃ³digo y procesos de negocio
- **EvaluaciÃ³n de Impacto**: Proporcionar anÃ¡lisis sobre las consecuencias potenciales de decisiones y cambios
- **IntegraciÃ³n Seamless**: Conectar con sistemas de gestiÃ³n de contenido y repositorios de cÃ³digo
- **IA Local**: Utilizar modelos de IA ejecutados localmente para mayor privacidad y control

## ğŸ—ï¸ Arquitectura

### Diagrama de Arquitectura del Sistema

```mermaid
graph TB
    %% Cliente y Frontend
    Client["ğŸŒ Cliente Web/API"] 
    
    %% Capa de AplicaciÃ³n
    subgraph "ExpertMind Application"
        NestJS["ğŸš€ NestJS Backend"]
        WSGateway["ğŸ”Œ WebSocket Gateway"]
        RESTApi["ğŸ“¡ REST API"]
    end
    
    %% Servicios Principales
    subgraph "Core Services"
        OllamaService["ğŸ§  Ollama Service"]
        MCPService["ğŸ”— MCP Client Service"]
        EnhancedService["âš¡ Enhanced Ollama Service"]
    end
    
    %% Infraestructura Externa
    subgraph "External Infrastructure"
        OllamaServer["ğŸ¤– Ollama Server\n(phi3:mini, tinyllama)"]
        GitHubAPI["ğŸ“‚ GitHub API"]
        MCPServer["ğŸ”„ MCP GitHub Server"]
    end
    
    %% ContainerizaciÃ³n
    subgraph "Docker Environment"
        DockerCompose["ğŸ³ Docker Compose"]
        OllamaContainer["ğŸ“¦ Ollama Container"]
    end
    
    %% Flujo de datos
    Client --> NestJS
    Client -.-> WSGateway
    NestJS --> RESTApi
    NestJS --> OllamaService
    NestJS --> MCPService
    
    OllamaService --> EnhancedService
    EnhancedService --> OllamaServer
    MCPService --> MCPServer
    MCPServer --> GitHubAPI
    
    DockerCompose --> OllamaContainer
    OllamaContainer --> OllamaServer
    
    %% Estilos
    classDef clientStyle fill:#e1f5fe
    classDef appStyle fill:#f3e5f5
    classDef serviceStyle fill:#e8f5e8
    classDef infraStyle fill:#fff3e0
    classDef dockerStyle fill:#f1f8e9
    
    class Client clientStyle
    class NestJS,WSGateway,RESTApi appStyle
    class OllamaService,MCPService,EnhancedService serviceStyle
    class OllamaServer,GitHubAPI,MCPServer infraStyle
    class DockerCompose,OllamaContainer dockerStyle
```

### Diagrama de Componentes

```mermaid
flowchart LR
    %% MÃ³dulos principales
    subgraph "ğŸ“± Application Layer"
        AppModule["App Module"]
        MainEntry["Main.ts\n(Bootstrap)"]
    end
    
    subgraph "ğŸ§  Ollama Module"
        OllamaController["Ollama Controller\n(REST Endpoints)"]
        OllamaService["Ollama Service\n(Basic AI Operations)"]
        EnhancedOllama["Enhanced Ollama Service\n(Advanced Logic)"]
    end
    
    subgraph "ğŸ”— MCP Module"
        MCPClient["MCP Client Service\n(Protocol Handler)"]
        GitHubMCP["GitHub MCP Server\n(Repository Integration)"]
        MCPInterfaces["MCP Interfaces\n(Type Definitions)"]
    end
    
    subgraph "ğŸŒ Communication Layer"
        RESTEndpoints["REST API\n(/api/*)"]
        WebSocketGW["WebSocket Gateway\n(Real-time)"]
    end
    
    subgraph "ğŸ§ª Testing Layer"
        UnitTests["Unit Tests\n(*.spec.ts)"]
        E2ETests["E2E Tests\n(test/)"]
        TestConfig["Jest Configuration"]
    end
    
    subgraph "ğŸ³ Infrastructure"
        DockerConfig["Docker Compose\n(Ollama Services)"]
        BuildConfig["Build Configuration\n(tsconfig, nest-cli)"]
        PackageConfig["Package Management\n(package.json)"]
    end
    
    %% Conexiones principales
    MainEntry --> AppModule
    AppModule --> OllamaController
    AppModule --> MCPClient
    
    OllamaController --> OllamaService
    OllamaService --> EnhancedOllama
    
    MCPClient --> GitHubMCP
    MCPClient --> MCPInterfaces
    
    OllamaController --> RESTEndpoints
    AppModule --> WebSocketGW
    
    %% Testing connections
    UnitTests -.-> OllamaService
    UnitTests -.-> MCPClient
    E2ETests -.-> AppModule
    
    %% Infrastructure connections
    DockerConfig -.-> EnhancedOllama
    BuildConfig -.-> AppModule
    PackageConfig -.-> MainEntry
    
    %% Estilos de componentes
    classDef appLayer fill:#e3f2fd
    classDef ollamaModule fill:#e8f5e8  
    classDef mcpModule fill:#fff3e0
    classDef commLayer fill:#f3e5f5
    classDef testLayer fill:#fce4ec
    classDef infraLayer fill:#f1f8e9
    
    class AppModule,MainEntry appLayer
    class OllamaController,OllamaService,EnhancedOllama ollamaModule
    class MCPClient,GitHubMCP,MCPInterfaces mcpModule
    class RESTEndpoints,WebSocketGW commLayer
    class UnitTests,E2ETests,TestConfig testLayer
    class DockerConfig,BuildConfig,PackageConfig infraLayer
```

### Stack TecnolÃ³gico

- **Backend**: NestJS con TypeScript
- **IA Local**: Ollama (phi3:mini, tinyllama)
- **Protocolo**: MCP (Model Context Protocol)
- **IntegraciÃ³n**: GitHub API
- **ContainerizaciÃ³n**: Docker & Docker Compose
- **Testing**: Jest
- **WebSockets**: Para comunicaciÃ³n en tiempo real

### Estructura del Proyecto

```
expertmind/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ mcp/                    # MÃ³dulo MCP para GitHub
â”‚   â”‚   â”œâ”€â”€ github-mcp-server.ts
â”‚   â”‚   â”œâ”€â”€ mcp-client.service.ts
â”‚   â”‚   â””â”€â”€ interfaces/
â”‚   â”œâ”€â”€ ollama/                 # MÃ³dulo Ollama para IA
â”‚   â”‚   â”œâ”€â”€ ollama.service.ts
â”‚   â”‚   â”œâ”€â”€ enhanced-ollama.service.ts
â”‚   â”‚   â””â”€â”€ ollama.controller.ts
â”‚   â”œâ”€â”€ app.module.ts
â”‚   â””â”€â”€ main.ts
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ docker-compose.yml     # ConfiguraciÃ³n de Ollama
â”œâ”€â”€ test/                      # Tests unitarios y e2e
â””â”€â”€ dist/                      # Build de producciÃ³n
```

## ğŸš€ Inicio RÃ¡pido

### Prerrequisitos

- Node.js 18+ 
- Docker y Docker Compose
- Git

### InstalaciÃ³n

1. **Clonar el repositorio**:
```bash
git clone <tu-repositorio>
cd expertmind
```

2. **Instalar dependencias**:
```bash
npm install
# o
yarn install
```

3. **Configurar variables de entorno**:
```bash
cp .env.example .env
# Editar .env con tus configuraciones
```

4. **Iniciar servicios con Docker**:
```bash
# Levantar Ollama y descargar modelos
npm run docker:up

# Verificar que los servicios estÃ©n corriendo
npm run docker:status

# Descargar modelos de IA (phi3:mini y tinyllama)
npm run docker:pull-models
```

5. **Iniciar la aplicaciÃ³n**:
```bash
# Desarrollo
npm run start:dev

# ProducciÃ³n
npm run build
npm run start:prod
```

## ğŸ› ï¸ Scripts Disponibles

### Desarrollo
```bash
npm run start:dev          # Modo desarrollo con hot-reload
npm run start:debug        # Modo debug
npm run build              # Compilar para producciÃ³n
npm run lint               # Linter
npm run format             # Formatear cÃ³digo
```

### Testing
```bash
npm run test               # Tests unitarios
npm run test:watch         # Tests en modo watch
npm run test:cov           # Tests con coverage
npm run test:e2e           # Tests end-to-end
```

### Docker
```bash
npm run docker:up          # Levantar servicios
npm run docker:down        # Bajar servicios
npm run docker:logs        # Ver logs
npm run docker:rebuild     # Reconstruir contenedores
npm run docker:pull-models # Descargar modelos de IA
npm run docker:list-models # Listar modelos disponibles
```

## ğŸ“Š Funcionalidades Principales

### 1. AnÃ¡lisis de DocumentaciÃ³n
- Procesamiento de archivos markdown, cÃ³digo fuente y documentaciÃ³n tÃ©cnica
- ExtracciÃ³n de conocimiento desde repositorios de GitHub
- AnÃ¡lisis contextual de proyectos completos

### 2. Consultas Inteligentes
- Respuestas a preguntas especÃ­ficas sobre el negocio
- AnÃ¡lisis de cÃ³digo y arquitectura
- Recomendaciones basadas en mejores prÃ¡cticas

### 3. EvaluaciÃ³n de Impacto
- AnÃ¡lisis de consecuencias de cambios propuestos
- EvaluaciÃ³n de riesgos y oportunidades
- Sugerencias de mejoras y optimizaciones

### 4. IntegraciÃ³n Flexible
- API REST para integraciÃ³n con sistemas externos
- WebSockets para comunicaciÃ³n en tiempo real
- Compatible con sistemas de gestiÃ³n de contenido (CMS)

## ğŸ”§ ConfiguraciÃ³n

### Variables de Entorno

```env
# AplicaciÃ³n
PORT=3000
NODE_ENV=development

# Ollama
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=phi3:mini

# GitHub (opcional)
GITHUB_TOKEN=tu_token_aquÃ­
GITHUB_REPO=usuario/repositorio
```

### Modelos de IA Soportados

- **phi3:mini**: Modelo ligero y eficiente para consultas generales
- **tinyllama**: Modelo ultra-compacto para respuestas rÃ¡pidas
- **Personalizable**: Soporte para otros modelos de Ollama

## ğŸ” Casos de Uso

### Para Desarrolladores
- AnÃ¡lisis de calidad de cÃ³digo
- DocumentaciÃ³n automÃ¡tica
- DetecciÃ³n de patrones y antipatrones
- Sugerencias de refactoring

### Para Product Managers
- AnÃ¡lisis de impacto de features
- EvaluaciÃ³n de decisiones tÃ©cnicas
- ComprensiÃ³n de dependencias
- EstimaciÃ³n de esfuerzos

### Para Equipos de Negocio
- Respuestas sobre procesos documentados
- AnÃ¡lisis de polÃ­ticas y procedimientos
- EvaluaciÃ³n de propuestas de cambio
- GeneraciÃ³n de reportes contextuales

## ğŸš§ Roadmap

### Fase 1 (Actual)
- [x] IntegraciÃ³n bÃ¡sica con Ollama
- [x] ConexiÃ³n con GitHub via MCP
- [x] API REST funcional
- [x] ContainerizaciÃ³n con Docker

### Fase 2 (PrÃ³ximo)
- [ ] Interfaz web para consultas
- [ ] IntegraciÃ³n con mÃºltiples fuentes de datos
- [ ] Sistema de plugins
- [ ] MÃ©tricas y analytics

### Fase 3 (Futuro)
- [ ] IntegraciÃ³n con CMS populares
- [ ] Soporte para mÃºltiples idiomas
- [ ] IA conversacional avanzada
- [ ] Dashboards ejecutivos

## ğŸ“š DocumentaciÃ³n Adicional

- [GuÃ­a de Desarrollo](./docs/development.md)
- [API Reference](./docs/api.md)
- [ConfiguraciÃ³n Avanzada](./docs/configuration.md)
- [Troubleshooting](./docs/troubleshooting.md)

## ğŸ¤ ContribuciÃ³n

1. Fork el proyecto
2. Crea una rama para tu feature (`git checkout -b feature/nueva-funcionalidad`)
3. Commit tus cambios (`git commit -am 'Agrega nueva funcionalidad'`)
4. Push a la rama (`git push origin feature/nueva-funcionalidad`)
5. Abre un Pull Request

## ğŸ“„ Licencia

Este proyecto es privado y no tiene licencia pÃºblica.

## ğŸ“ Soporte

Para soporte tÃ©cnico o preguntas sobre el proyecto, contacta al equipo de desarrollo.

---

**ExpertMind** - Transformando informaciÃ³n en inteligencia de negocio ğŸš€